\justifying
\textbf{Цель работы:}
Научиться находить минимум целевой функции, заданной в области.

\textbf{Задание:}
Вычислить минимум целевой функции
\begin{equation}\label{main_equation}
    z = f(x,y) = \frac{1}{2} A x ^ 2 + B x y + \frac{1}{2} C y ^ 2 - D x - E y + F,
\end{equation}
заданной в области $x ^ 2 + y ^ 2 \leq R$ аналитическим способом, с помощью метода градиентного спуска и метода Ньютона.

\textbf{Ход работы:}

Аналитическим способом точку минимума можно получить, решив систему уравнений:
\begin{equation}
    \begin{cases}
    \frac{\partial}{\partial x} f(x, y) = 0 \\
    \frac{\partial}{\partial y} f(x, y) = 0.
    \end{cases}
\end{equation}
Используя выражение~(\ref{main_equation}), получаем точку минимума целевой функции с координатами:
\begin{equation}
    \begin{cases}
    x_{min} = \frac{c d - b e}{a c - b ^ 2} \\
    y_{min} = \frac{a e - b d}{a c - b ^ 2}.
    \end{cases}
\end{equation}
Подставим следующие числа: $A = 1, B = 0.1, C = 3, D = 2, E = 4, F = 0.5$. Тогда числовые значения координат буду следующими:
\begin{equation}
    \begin{cases}
    x_{min} = 1.873 \\
    y_{min} = 1.271.
    \end{cases}
\end{equation}

Метод градиентного спуска зададим по следующей формуле:
\begin{equation}
    (x^{(n + 1)}, y^{(n + 1)}) = (x^{(n)}, y^{(n)}) - k \cdot \nabla f(x^{(n)}, y^{(n)}),
\end{equation}
где $(x^{(n)}, y^{(n)})$ -- координаты функции $f$ на $n$ -- ом шаге, а $k$ -- темп обучения.

Градиентный спуск с абсолютной погрешность $10^{-4}$, темпом обучения $0.25$ и начальным приближением $[1, 1]$ сходится к точке с координатами $(1.873, 1.271)$ за 28 итераций.

Применим метод Ньютона к уравнению:
\begin{equation}
    \nabla f(x, y) = 0.
\end{equation}
Тогда итерационный процесс будет следующим:
\begin{equation}
    (x^{(n + 1)}, y^{(n + 1)}) = (x^{(n)}, y^{(n)}) - \frac{\nabla f(x^{(n)}, y^{(n)})}{\nabla ^ 2 f(x^{(n)}, y^{(n)})},
\end{equation}

Метод Ньютона с абсолютной погрешность $10^{-4}$ и начальным приближением $[1, 1]$ сходится к точке с координатами $(1.873, 1.271)$ за 15 итераций.

\textbf{Выводы:}

В настоящей лабораторной работе был найден минимум целевой функции тремя способами: аналитическим способом, методом градиентного спуска и методом Ньютона. Сравнивая итерационные процессы, можно сказать, что метод Ньютона сходится быстрее. Но данный факт выполняется не всегда. Более того, варьируя темп обучения, можно добиться более быстрой сходимости метода градиентного спуска.